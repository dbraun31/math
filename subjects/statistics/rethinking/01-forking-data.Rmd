# Small Worlds and Large Worlds

He opens the chapter by using the example of Christopher Columbus to illustrate
the difference between our ideas (Columbus thinking the world was small enough to sail to Asia)
and reality (Columbus landing in the Western hemisphere).

A **small world** is a self-contained logical model of the world. We need to be 
able to verify the model's logic, ensuring it performs as expected under favorable conditions. 
He says Bayesian models have reasonable claims to optimality: no model could make better 
use of the data, assuming the model of reality is an accurate description.

The **large world** is the broader context in which the model is deployed. The broader
context often presents surprises. And the small world is always an imperfect representation
of the larger world.

**Aims of the chapter.** Here we're concerned with the small world. We'll focus
on probability theory, then see how Bayesian inferences arises naturally from this 
perspective. We then see Bayesian statistical models; then how to animate the model
to produce estimates.

*A short note about how real organisms use heuristics as adaptive shortcuts and
aren't fully "Bayesian" in their logic; nor should they be.*

## The garden of forking data

The image is one where many roads branch off into all possibilities. In order
to make good inference about what actually happened, we need to consider everything
that could've happened. As we learn about what actually happened, some of the hypothetical
possibilities are pruned. **We arrive at a quantitative ranking of hypotheses.** 
This approach guarantees the best *small world* solution (although not necessarily
the best big world solution).

Example:

### Counting possibilities

A bag contains four marbles, either blue or white. The bag is sampled *with* 
replacement N times. Each possible state (of which there are five) of the true 
count of blue / white marbles will be referred to as the five *conjectures*.
We need to figure out which conjecture is most plausible.

We start with N = 3, and observe blue, white, blue. These are the data.

Now we start with the conjecture that the bag actually contains blue, white, 
white, white, and we visualize (just look at figures starting on p. 21; I 
won't try to bring them in here). But basically we chart out all possible 
draws that could have happened on the first draw for our given conjecture,
which again is one blue and three white. Then from each of those endpoints, 
we again chart all possibilities for the outcome of the *second* draw, and so forth,
until we arrive at a big forking garden. Any pathway through the garden that 
violates our observation (such as all white or all blue) can be eliminated. Ah,
and additionally, the sequence of the paths matter, such that the first element 
in the path has to match the first observation, and so on. So, in the first layer, 
we actually eliminate all the white marbles. We're not concerned with which exact
path the data took, we're only concerned with how many plausible pathways remain
for that given conjecture.

We then repeat the same procedure for the remaining conjectures. The all white
and all blue conjectures are easy---they get fully eliminated. So we're really
doing a counting comparison on the remaining three (including the one we walked
through above). We tally up the valid pathways:

| Conjecture | Ways to produce |
|:-----------:|:---------------:|
| Four white | $0 \times 4 \times 0 = 0$ |
| One blue   | $1 \times 3 \times 1 = 3$ |
| Two blue   | $2 \times 2 \times 2 = 8$ |
| Three blue | $3 \times 1 \times 3 = 9$ |
| Four blue | $4 \times 0 \times 4$ |

The math trick here is just to take the number of possibilities in each layer
and multiply them together (same thing as counting).

These counts are only part of the solution to rate the relative plausibility of
each conjecture. But we also need to know how many ways each conjecture itself
could be realized.[^1]

[^1]: He's obviously talking about priors here, but it's not yet totally clear 
to me how that applies in this situation. Ah, introduce the next section...


### Combining other information

This is all about priors. We could have some background knowledge about the 
relative plausibility of the conjectures (ie, how many of which type of marbles
are in the bag). 

So we can imagine that we do the full procedure above and arrive at the counts
in the table. And then we want to repeat that procedure. There's no sense 
throwing out the past data we've collected about what's in the bag; instead,
we combine that information with our new observations to determine the 
plausibility of the conjectures.

So we repeat the procedure for this new round of data collection (assuming
these draws are each independent from each other and from the previous round
of data collection). We arrive at our counts once again, and this time we 
multiply the new counts by the old counts to arrive at a new plausibility estimate.

Example if one new observation was a blue marble:

| Conjecture | Ways to produce | Prior counts | New count |
|:-----------:|:--------------:|:------------:|:----------:|
| Four white | 0 | 0 | $0 \times 0 = 0$ | 
| One blue | 1 | 3 | $3 \times 1 = 3$ |
| Two blue | 2 | 8 | $2 \times 8 = 16$ |
| Three blue | 3 | 9 | $3 \times 9 = 18$ |
| Four blue | 4 | 0 | $0 \times 4 = 0$ |

Multiplication here again is just a shortcut for adding up all the total valid
"pathways through the garden" for that conjecture.

*My note:* Then *I think* what you do is turn the new counts into a probability
distribution by dividing by the "probability of the data"; in this case, the sum
of all counts (eg, for two blue: $16 / (0 + 3 + 16 + 18 + 0) = 0.432$). And I guess
that's (at least for discrete events) the marginal probability.

p. 27