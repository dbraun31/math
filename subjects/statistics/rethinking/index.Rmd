--- 
title: "Statistical Rethinking Notes"
author: "Dave Braun"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::gitbook,
  set in the _output.yml file.
link-citations: yes
github-repo: rstudio/bookdown-demo
---
```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

# The Golem of Prague

This chapter was largely an introduction to the major themes of the book. The major
topics covered are:

1. Bayesian data analysis
2. Model comparison
3. Multilevel models
4. Graphical causal models

Maybe the most interesting discussion was around Figure 1.2 and all the problems 
in inference that arise due to the relationships between hypotheses, process models,
and statistical models. Each can imply more than one of the other, and so a 
strict falsification approach is problematic. 

Each chapter comes along with a YouTube video lecture. I'll take more real-time
notes along with this first lecture below. The YouTube lecture for chapter one
is [here](https://www.youtube.com/watch?v=FdnMWdICdRs&list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus&index=1).

## Lecture notes

He's arguing that the argument between Bayesian vs. frequentist statistics is 
only had by boomer statisticians. The real discussion is about causal inference.
For statistical models to produce scientific insight, they require
additional **scientific (causal) models**. The causes for the data don't come from the data,
they must come from theory.

### What is causal inference?

Two key aspects to understanding causal inference:

1. **Prediction of intervention.** Knowing a cause means being able to predict the 
consequences of an *intervention*. More than just prediction per se.
2. **Imputation of missing observations.** Knowing a cause means being able to construct
unobserved counterfactual outcomes. Makes me think of simulating a model where
you can play out many different scenarios.

Even "purely" descriptive research needs to consider causal modeling. Any time
you're trying to draw inferences about a population from a sample, you need to
consider factors that might have caused the sample to be the way that it is.

#### Directed acyclic graphs (DAGs)

These are essentially box-and-arrow causal models of a process. 

```{r echo=FALSE}
library(DiagrammeR)
mermaid("
    graph LR
    
    X --> Y
    A --> X
    C --> X
    C --> Y
    A --> C
    B --> C
    B --> Y
")
```

We're saying that changing $X$ will induce some change in $Y$; changing $Y$, 
however, will not impact $X$. And we make no assumptions about the *nature* of the
relationship (eg, whether it is linear). The goal is to see what insight can be gleaned
with these relatively few assumptions, before making more assumptions about functional
relationships (which affords much more scientific power), and indeed DAGs are a good 
precursor to more specific process models.

**Interim summary**
I think a major point here is that null hypothesis rejection is often insufficient because researchers
fail to consider other possible accounts that could explain the same pattern of results.
He goes through many examples of this. To support one hypothesis, one needs to 
draft process models of both the main hypothesis and competing hypotheses, which
will force researchers to make many assumptions, and will reveal where hypotheses
make overlapping and diverging predictions.

**Looking forward**

* Research requires more than just null hypothesis testing.
* We need generative causal models
    * Start with DAGs but need to go beyond. We need a model that data can be simulated from.
* Then write statistical models
    * First analyze synthetic data to make sure important questions can be answered
    * Then introduce real data to the model / statistical test
    
YES! This guy is speaking my language!

Bayes might be perceived as overkill sometimes, but, for testing predictions from 
generative models, Bayes is extremely well equipped to handle many of the challenges
that come up from routine issues in data.

Steps:

1. Theoretical estimand
2. Scientific (causal) model(s)
3. Use 1 & 2 to build statistical model(s)
4. Simulate from 2 to validate 3 yields 1
5. Analyze real data
